#+TITLE: Report of bioninformatic project
#+AUTHOR: Stefano Taverni @@latex:\\@@ stefano.taverni@studenti.unimi.it
#+DATE: \today

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper, 12pt]
#+LATEX_HEADER:
#+OPTIONS: toc:nil

#+begin_abstract
The objective of the project is the prediction of active regulatory regions in
cell line K562 through the development of different kind of deep neural
networks. The main focus of the project is not the overtaking of
state-of-the-art network models, but how to correctly construct such models, the
setup of the experimental environment and how to obtain statistically
significant and reliable results.
#+end_abstract

* Introduction
  All known organism generate their macromolecules, such as proteins, through
  the gene expression process. During this process, the information stored in a
  gene are synthesised in an end product, like protein or non-codind RNA. All
  the steps in the gene expression may be regulated to increase or decrease the
  production of specific gene products: this mechanism is called gene
  regulation. In particular, the regulation may take place during the
  transcription, the phase in which a segment of DNA is copied into RNA. During
  the transcription, two important Cis-regulatory elements (CREs), a subset of
  non-coding DNA, play fundamental roles: first of all, there is the promoter, a
  sequence of DNA located near the transcription start site of a gene, and it's
  responsable for the initiation of the transcription process; typically it can
  be about 100-1000 base pair long, but this length vary from differnt organisms
  and also from diffent genes. Then, there is the enhancer, which is also a
  sequence of DNA which can be tied up by another protein to increase the
  likelihood that transcription occurs; it's usually 50-1500 base pair long, but
  they are locate far away from the start site, even 1 milion base pair
  away.\cite{regseq, geneexpr, promoters, enhancers, transcription, cis}

  The identification of active regulatory region in non-coding DNA sequences is
  an extremely important work because it allow us to forecast if a genetic
  variant could be pathogenic or not: indeed, if a mutation happens in an
  incative DNA region, then it is less likely to be potentially harmful. But if
  a genetic variant is located in an active regulatory region, there are high
  changes of serial severe human diseases.

  Since 98% of our human genome is composed by non-coding DNA regions, the
  prediction of active regulatory region is a complex task to be carried out by
  humans. The empoling of artifical intelligence methods is nowdays essential to
  identify the activation status and the location of these regions. Notable and
  promising machine learning methods developed so far in this field are:
  + DeepEnhancer, a convolutional neural network trained to distinguish
    enhancers from background genomic sequences. \cite{deepenhancer}
  + DECRES, a deep learning model specifically created for the identification of
    enhancer and promoter regions in the human genome. \cite{decres}
  + BiRen, a deep-learning-based hybrid architecture which predicts enhancers
    using the DNA sequence alone. \cite{biren}

  Given the complexity of such task, this project doens't aim to outperform
  state-of-the-art models, but rather it focuses on how to correctly develop
  differnt kind of neural network models from the ground up, starting from the
  retrival of the epigenomic and sequence data to all the preprocessing steps
  required to properly train the models.

* Models

  Four different kinds of predictor have been developed, in particular one
  ensamble method and the neural network:
  1. A random forest classifier, with the following paramters:
     - $100$ trees in the forest;
     - each tree has a maximum depth of 5 nodes;
     - the minimum number of samples that should be routed to a leaf node is set
       to $100$;
     - Gini function as split quality metric;
     - weighted classes based on their frequencies in the subsamples;
     These hyperparemter aren't meant to create an outperforming classifier, but
     merely produce a simple and low footprint predictor to later compare more
     complex models.
  2. A feed-forward neural network with the following structure:
     | Units | Activation function |
     |-------+---------------------|
     |   256 | ReLU                |
     |   128 | ReLU                |
     |    64 | ReLU                |
     |     1 | Sigmoid             |

     The structure of this model is the one proposed in \cite{fixedffnn}, in
     particular the one with the optimize paramters given by the Bayesian
     optimization. The weight adjustament function is a Stochastic Gradient
     Descent technique, with learning rate equals to $0.1$ and batch size of
     $1024$ samples.
  3. A Convolutional Neural Network, built as follow:
     | Type                 | Activation | Units | Kernel | Notes    |
     |----------------------+------------+-------+--------+----------|
     | Conv1D               | ReLU       |    64 | 5      | -        |
     | Conv1d               | ReLU       |    32 | 4      | -        |
     | MaxPooling           | -          |     - | -      | Size=2   |
     | Conv1d               | ReLU       |    32 | 4      | -        |
     | MaxPooling           | -          |     - | -      | Size=2   |
     | GlobalAveragePooling | -          |     - | -      | -        |
     | Dense                | ReLU       |    64 | -      | -        |
     | Dropout              | -          |     - | -      | Rate=0.2 |
     | Dense                | ReLU       |    32 | -      | -        |
     | Dropout              | -          |     - | -      | Rate=0.2 |
     | Dense                | Sigmoid    |     1 | -      | -        |

     The optimazing function is a Nadam procedure with learning rate equals to
     $0.002$.
  4. A Multi-Modal Neural Network, which allows to combine two differnt models
     and with different input spaces into one single model. The two previous
     neural networks are combined in the input layer, they compute in parallel
     thoughout the hidden layers until they reach the last hidden layer, which
     is concatenated to create a single hidden layer for the Multi-Modal model.
     Then, the joint hidden layer is connected to a Dense layer with 64 units
     and ReLU activation function, ending in a Dense layer with 1 unit and
     Sigmoid activation function. This model can be constructed in two differnt
     way: suppling the pre-trained neural networks, perfoming only the fine
     tuning of the last hidden layer; or giving as input the shape of the input
     data, creating the internal neural networks from scratch and then
     concatenate them.  In both cases, a Nadam optimizing procedure is used to
     tune the weights.

* Considered tasks

  This work studies two different tasks: active enhancers vs inactive enhancers
  (AEvsIE) and active promoters vs incative promoters (APvsIP) in the cell line
  K562 from HG38 dataset. This means that the predictors should be trained in
  order to identify which region of the non-coding DNA has an actual function of
  promoter or enhancer, avoiding misclassifications.

  The HG38 is the reference human genome released on December 2013, and its
  function is to represent an example of how the set of genes are distributed in
  the human beings \cite{genome}. Where the K562 cell line was derived from
  leukemic cells obtained in December 1970 from a pleural effusion of a
  53-year-old female suffering from chronic myelogenous leukemiafor about 4
  years \cite{k562}.
* Dataset source

  Two differnt kinds of dataset are used to build the predictors.

  The epigenomic datasets, for instance the enanchers and promoters, are
  retrived thanks to the automated pipeline[fn:1] which downloads all the data from
  ENCODE, a consortium providing a comprehensive parts list of functional
  elements in the human genome, including elements that act at the protein and
  RNA levels, and regulatory elements that control cells and circumstances in
  which a gene is active \cite{encode}. After that, the regulatory elements are
  linked to their activation status, retriving the labels from FANTOM, a
  consortium which provides functional annotation for the mammalian genome
  \cite{fantom}.

  The other dataset is directly the HG38 human genome assembly, retrived by
  another automated pipeline[fn:2] which fetches and elaborates the information
  directly from \cite{genome}.

* Preprocessing

  The applied preprocessing steps are:
  - binarization of the labels, because they are downloaded as boolean values,
    and to perform some data visualization they are transformed into intger
    values;
  - imputation of missing values via the median strategy, a robust statistical indactor;
  - scaling of the features through sklearn RobustScaler[fn:3], which removes
    the median and scales the data according to the quantile range, making this
    scaling robust to outliers.
  These preprocessing steps are wrapped in a sklearn Pipeline[fn:4], allowing
  an efficient and convient way to compute the statistic values only on the
  training set, and the scales separately the test set, avoding any kind of data
  leakage.
* Data correlation and distribution

  The dataset of AEvsIE presents a strong class imbalance: indeed, only
  $0.633\%$ of the samples leads to an active region. This situation doens't
  appear in the APvsIP task, where $21.657\%$ of the sample are active
  region. This situation is better visualized in figure [[fig:balance]].

  #+caption: class balance for APvsIP (right) and AEvsIE (left) tasks
  #+name: fig:balance
  [[./img/class-balance.png]]

  Note that a region is considered active when it has a TPM value at least of 1,
  while it is labeled as inactive with values lower than 1.

  Concerning the feature correlation, the Pearson correlation coefficient has
  been computed between each feature: this is useful to identify any kind of
  linear correlation between the features, in order to remove the ones with
  higher correlation (0.95 threshold score, with p value equals to 0.01),
  favoring the features with more entropy. Figures [[fig:promcorr]] [[fig:enhacorr]]
  show the top 3 correlated features for both datasets.

  #+caption: three most correlated features in APvsIP dataset
  #+name: fig:promcorr
  [[./img/promoters-feature-corr.png]]
  #+caption: three most correlated features in AEvsIE dataset
  #+name: fig:enhacorr
  [[./img/enhancers-feature-corr.png]]

  It is also necessary to investigate which feature has no correlation with the
  output: this can help to reduce the burden of useless feature in the
  classification tasks. The study of this kind of correlation has been done
  thanks to three differnt coefficients. First of all, the Pearson coefficient
  helps to identify any linear relation between the single feature and the
  output values; then, the Spearman correlation coefficient identifies monotonic
  relations, both linear or not. Finally, the maximal information coefficient
  (MIC) is computed only on the subset of variables discovered by the previuos
  two metrics, in order to filter out any misleading classification, and also to
  save some time. This phase leads to the identification of the following
  features as non correlated with the output:
  + promoters, THRAP3, NCOA4
  + enhancers, H3K36me3, XRCC3, SRSF7, ZNF318, SNRNP70, ZNF830, THRA, SAFB2,
    U2AF2, STAG1, RBM17, ZKSCAN3, NFE2L1, HNRNPH1, RBM34, ZNF408, FOXJ3, ILK,
    ZNF280A, whole-genome shotgun bisulfite sequencing, ZNF778, DLX4, FOXA1,
    MCM7, RBM15, PCBP2, ZBTB8A


* Feature selection
Taking inspiration from \cite{rfe}, even though the application field is
slightly different from the one we are facing in this paper, the feature
selection method applied is a Recursive Feature Elimination (RFE) with Support
Vector Machine (SVM) as internal classifier. RFE is an iterative process of
backward feature elimination, and works by taking an external classifier and
training it on the dataset; then the features are ranked according to a ranking
criterion, and finally the feature with the smallest rank are removed. RFE can
work with a subset of feature greater than 1 at each step, removing more than
one feature at each iteration. The process ends as soon as the required number
of features is reached.
As interal classifier, SVM is a linear classifier that creates an hyperplane or
a set of hyperplanes in a higher dimensional space. It chooses the hyperplane
that maximize the distance between the two classes, reducing the generalization
error. SVM can be adapted to cope with nonlinear separable dataset, by choosing
a kernel function to transform the feature space.

In this work, the used implementations of these methods are taken from
Sklean[fn:5][fn:6], where RFE is tuned by choosing 250 features as number of
feature to select, and 10 features to remove at each iteration.  While SVM is
created with a radial basis function as kernel, log loss as loss function and L1
regularization.

* Data visualization

* Holdouts
* Result analysis

* Results


* Bibliography

  \begin{thebibliography}{99}
  \bibitem{regseq}
    Wikipedia, the Free Encyclopedia,
    Regulatory Sequence,
    \texttt{https://en.wikipedia.org/wiki/Regulatory\_sequence},

  \bibitem{geneexpr}
    Wikipedia, the Free Encyclopedia,
    Regulation of gene expression,
    \texttt{https://en.wikipedia.org/wiki/Regulation\_of\_gene\_expression},

  \bibitem{promoters}
    Wikipedia, the Free Encyclopedia,
    Promoters,
    \texttt{https://en.wikipedia.org/wiki/Promoter\_(genetics)},

  \bibitem{enhancers}
    Wikipedia, the Free Encyclopedia,
    Cis-regulatory element,
    \texttt{https://en.wikipedia.org/wiki/Enhancer\_(genetics)}

  \bibitem{transcription}
    Wikipedia, the Free Encyclopedia,
    Transcription,
    \texttt{https://en.wikipedia.org/wiki/Transcription\_(biology)},

  \bibitem{cis}
    Wikipedia, the Free Encyclopedia,
    Cis-regulatory element,
    \texttt{https://en.wikipedia.org/wiki/Cis-regulatory\_element}

  \bibitem{deepenhancer}
    Min, X., Zeng, W., Chen, S. et al. Predicting enhancers with deep
    convolutional neural networks. BMC Bioinformatics 18, 478 (2017).
    \texttt{https://doi.org/10.1186/s12859-017-1878-3}

  \bibitem{decres}
    Li, Y., Shi, W. \& Wasserman, W.W. Genome-wide prediction of cis-regulatory
    regions using supervised deep learning methods. BMC Bioinformatics 19, 202
    (2018).
    \texttt{https://doi.org/10.1186/s12859-018-2187-1}

  \bibitem{biren}
    Bite Yang, Feng Liu, Chao Ren, Zhangyi Ouyang, Ziwei Xie, Xiaochen Bo,
    Wenjie Shu, \\BiRen: predicting enhancers with a deep-learning-based model
    using the DNA sequence alone, Bioinformatics, Volume 33, Issue 13, 1 July
    2017, Pages 1930–1936,
    \texttt{https://doi.org/10.1093/bioinformatics/btx105}

  \bibitem{fixedffnn}
    Luca Cappelletti, Alessandro Petrini, Jessica Gliozzo, Elena Casiraghi, Max
    Schubach, Martin Kircher, and Giorgio Valentini. Bayesian optimization
    improves tissue-specific prediction of active regulatory regions with deep
    neural networks. In Springer, editor, Bioinformatics and Biomedical
    Engineering, IWBBIO 2020, Lecture Notes in Computer Science, 2020.

  \bibitem{genome}
    Kent WJ, Sugnet CW, Furey TS, Roskin KM, Pringle TH, Zahler AM, Haussler
    D. The human genome browser at UCSC. Genome Res. 2002 Jun;12(6):996-1006.

  \bibitem{k562}
    Carmen B. Lozzio, Bismarck B. Lozzio, Human Chronic Myelogenous Leukemia
    Cell-Line With Positive Philadelphia Chromosome, Blood, Volume 45, Issue 3,
    1975, Pages 321-334, ISSN 0006-4971,
    \texttt{https://doi.org/10.1182/blood.V45.3.321.321}.

  \bibitem{encode}
    The ENCODE Consortium, 2021, Stanford University,
    \texttt{https://www.encodeproject.org/}

  \bibitem{fantom}
    FANTOM, 2014, RIKEN
    \texttt{https://fantom.gsc.riken.jp/}

  \bibitem{rfe}
    Guyon, I., Weston, J., Barnhill, S. et al. Gene Selection for Cancer
    Classification using Support Vector Machine.  Machine Learning 46, 389–422
    (2002).
    \texttt{https://doi.org/10.1023/A:1012487302797}

 
  \end{thebibliography}

* Footnotes

[fn:1] https://github.com/AnacletoLAB/epigenomic_dataset
[fn:2] https://github.com/LucaCappelletti94/ucsc_genomes_downloader
[fn:3] https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html
[fn:4] https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html
[fn:5] https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html
[fn:6] https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
